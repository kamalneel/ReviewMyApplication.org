# XCounselor Knowledge Base

This directory contains all the knowledge, rubrics, prompts, and examples that power the XCounselor AI admissions counselor.

## Structure

```
counselor-knowledge/
├── v1/                          # Version 1 of the counselor
│   ├── manifest.json            # Version metadata and file index
│   ├── requirements/            # Admission requirements by program
│   │   ├── college.md
│   │   ├── high-school.md
│   │   └── middle-school.md
│   ├── rubrics/                 # Evaluation criteria
│   │   ├── essay-evaluation.json
│   │   ├── academic-evaluation.json
│   │   ├── activity-evaluation.json
│   │   └── overall-evaluation.json
│   ├── prompts/                 # System prompts for the AI
│   │   ├── college-counselor.md
│   │   ├── high-school-counselor.md
│   │   └── middle-school-counselor.md
│   └── exemplars/               # Annotated examples
│       └── essay-examples.json
└── README.md                    # This file
```

## Versioning Strategy

Each major version of the counselor is a separate directory (`v1/`, `v2/`, etc.). This allows:

1. **A/B Testing**: Run different versions simultaneously
2. **Rollback**: Quickly revert to a previous version if issues arise
3. **Training Tracking**: Know which version generated each review
4. **Gradual Rollout**: Deploy new versions to a subset of users first

## File Types

### Manifest (`manifest.json`)
Metadata about the counselor version:
- Version number and changelog
- Supported programs (middle-school, high-school, college)
- Tested AI models
- File paths for all components

### Requirements (`requirements/*.md`)
Markdown documents containing comprehensive admission requirements:
- Student profile requirements
- Academic requirements
- Extracurricular expectations
- Essay requirements
- Document checklists

These are used to give the AI context about what matters for each program type.

### Rubrics (`rubrics/*.json`)
Structured evaluation criteria:
- **Categories**: What aspects to evaluate
- **Weights**: How important each category is
- **Scoring Levels**: Criteria for each score range
- **Red Flags**: Things that should trigger score penalties
- **Feedback Templates**: How to phrase feedback

### Prompts (`prompts/*.md`)
System prompts for each program type:
- Role definition (who is XCounselor)
- Evaluation framework
- Tone and approach guidelines
- Output format specifications
- Age-appropriate adjustments

### Exemplars (`exemplars/*.json`)
Annotated examples for reference:
- **Excellent Examples**: What great essays look like
- **Needs Work Examples**: Common problems with explanations
- **Annotations**: Why examples are good or bad
- **Improvement Suggestions**: How to fix common issues

## How the AI Uses This Knowledge

1. **System Prompt**: The appropriate `prompts/*.md` file is loaded as the AI's system prompt
2. **Context Injection**: Relevant rubric criteria are included in the prompt
3. **Few-Shot Examples**: Exemplars may be included to guide the AI's output
4. **Evaluation**: The AI uses rubrics to score each category
5. **Feedback Generation**: Templates guide the tone and structure of feedback

## Updating the Counselor

### Minor Updates (v1.0 → v1.1)
- Edit files in place
- Update `manifest.json` with new version and changelog
- Deploy Edge Function to pick up changes

### Major Updates (v1 → v2)
1. Copy `v1/` to `v2/`
2. Make significant changes
3. Update `manifest.json` with new version
4. Test with a subset of users
5. Gradually increase traffic to v2
6. Deprecate v1 when confident

## Adding a School-Specific Counselor

For premium school-specific counselors (e.g., MIT):

```
counselor-knowledge/
├── v1/                          # Generic counselor
└── schools/
    └── mit/
        └── v1/
            ├── manifest.json    # MIT-specific metadata
            ├── school-profile.json
            ├── essay-rubric.json
            ├── prompts/
            │   └── mit-counselor.md
            └── exemplars/
                └── mit-essays.json
```

## Training Loop

The reviews generated by each counselor version are stored in the database with `counselor_version` tracking. This enables:

1. **Performance Analysis**: Compare quality across versions
2. **Training Data Export**: Extract application + review pairs
3. **Improvement Identification**: Find where the counselor struggles
4. **Model Fine-Tuning**: Use data to train specialized models

### Export Training Data

```sql
SELECT 
    a.program_type,
    a.form_data,
    r.category_scores,
    r.feedback_text,
    r.decision,
    r.counselor_version
FROM applications a
JOIN reviews r ON a.id = r.application_id
WHERE r.counselor_version = 'v1.0.0'
ORDER BY r.created_at;
```

## Best Practices

1. **Keep prompts focused**: Each prompt should be clear about its role
2. **Update rubrics carefully**: Changes affect all evaluations
3. **Test with real examples**: Before deploying, test edge cases
4. **Track performance**: Monitor scores and user feedback by version
5. **Document changes**: Always update changelog in manifest

